{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 8\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VALID_BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "EMBEDDING_DIM = 100\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/cleaned_data/testing/random_names_eval.csv\"\n",
    "eval_path = \"data/cleaned_data/testing/random_names_eval.csv\"\n",
    "df_train = pd.read_csv(train_path,encoding='utf-8',usecols=[1,2])\n",
    "df_train.dropna()\n",
    "df_eval = pd.read_csv(eval_path,encoding='utf-8',usecols=[1,2])\n",
    "df_eval.dropna()\n",
    "df_train['name'].astype('str')\n",
    "df_eval['name'].astype('str')\n",
    "df_train['label'].astype('int')\n",
    "df_eval['label'].astype('int')\n",
    "X_train = df_train['name']\n",
    "y_train = df_train['label']\n",
    "X_test = df_eval['name']\n",
    "y_test = df_eval['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names, labels):\n",
    "        self.names = names\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = torch.tensor(self.names[idx]).type(torch.LongTensor)\n",
    "        label = torch.tensor(self.labels[idx]).type(torch.LongTensor)\n",
    "        return name, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,embedding_matrix):\n",
    "        super(LSTM,self).__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, 32, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(64, 16, bidirectional=True, batch_first=True)  # 64 because it's bidirectional\n",
    "        self.lstm3 = nn.LSTM(32, 8, bidirectional=True, batch_first=True)  # 32 because it's bidirectional\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(16, 16)  # 16 because it's bidirectional\n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader,model,optimizer,device):\n",
    "    \"\"\"\n",
    "    This is the main training function that trains model\n",
    "    for one epoch\n",
    "    :param data_loader: this is the torchdataloader\n",
    "    :param model: model(lstm model)\n",
    "    :param optimizer: optimizer Adam, SGD etc\n",
    "    :param device: this can be \"cuda\" or \"cpu\"\n",
    "    \"\"\"\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # go through the batches of data in data_loader:\n",
    "    for data in tqdm(data_loader):\n",
    "        names = data[\"names\"]\n",
    "        labels = data[\"labels\"]\n",
    "        # move the data to the device that we want to use\n",
    "        names = names.to(device,dtype=torch.long)\n",
    "        labels = labels.to(device,dtype=torch.float)\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make predictions from the models\n",
    "        predictions = model(names)\n",
    "        # loss\n",
    "        loss = nn.BCEWithLogitsLoss()(predictions,labels.view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def evaluate(data_loader,model,device):\n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    \n",
    "    model.eval()\n",
    "    for data in data_loader:\n",
    "        names = data[\"names\"]\n",
    "        labels = data[\"labels\"]\n",
    "        # move the data to the device that we want to use\n",
    "        names = names.to(device,dtype=torch.long)\n",
    "        labels = labels.to(device,dtype=torch.float)\n",
    "        predictions = model(names)\n",
    "        predictions = predictions.detach().cpu().numpy().tolist()\n",
    "        labels = data[\"labels\"].detach().cpu().numpy().tolist()\n",
    "        final_predictions.extend(predictions)\n",
    "        final_targets.extend(labels)\n",
    "        \n",
    "        \n",
    "    return  final_predictions,final_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index,embedding_dict):\n",
    "    \"\"\"\n",
    "    This function creates the embedding matrix\n",
    "    :param word_index: a dictionary of word: index_value\n",
    "    :param embedding_dict:\n",
    "    :return a numpy array with embedding vectors for all known words\n",
    "    \"\"\"\n",
    "    # intialize the embedding matrix \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM ))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "\n",
    "    print('Fitting tokenizer')\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # oov_token = \"<OOV>\"\n",
    "    # tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = MAX_LEN, oov_token=oov_token)\n",
    "    # tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    xtrain = tokenizer.texts_to_sequences(X_train)\n",
    "    xtest = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain,maxlen = MAX_LEN)\n",
    "    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest,maxlen = MAX_LEN)\n",
    "    \n",
    "    train_dataset = NameDataset(names=xtrain,labels=df_train.label.values)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=TRAIN_BATCH_SIZE,num_workers=2)\n",
    "\n",
    "    valid_dataset = NameDataset(names=xtest,labels=df_eval.label.values)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=VALID_BATCH_SIZE,num_workers=2)\n",
    "    \n",
    "    return valid_loader \n",
    "    # print(\"Load embeddings\")\n",
    "    # embedding_dict = load_vectors('data/glove.6B.100d.txt')\n",
    "    # embedding_matrix = create_embedding_matrix(tokenizer.word_index,embedding_dict)\n",
    "    # # create a torch device since we are using cuda\n",
    "    # \n",
    "    # print(device)\n",
    "    \n",
    "    # model = LSTM(embedding_matrix)\n",
    "    # model.to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)\n",
    "    \n",
    "    # print(\"Training model\")\n",
    "    # best_accuracy = 0\n",
    "    # early_stopping_counter = 0\n",
    "    # for epoch in range(1,EPOCHS+1):\n",
    "    #     train(train_loader,model,optimizer,device)\n",
    "    #     outputs,targets = evaluate(valid_loader,model,device)\n",
    "    #     outputs = np.array(outputs)>0.5\n",
    "    #     accuracy = metrics.accuracy_score(targets,outputs)\n",
    "    #     print(f\"EPOCH:{epoch}, Accuracy Score: {accuracy}\")\n",
    "    #     if accuracy>best_accuracy:\n",
    "    #         best_accuracy = accuracy\n",
    "    #     else:\n",
    "    #         early_stopping_counter +=1\n",
    "    #     if early_stopping_counter>2:\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer\n"
     ]
    }
   ],
   "source": [
    "val =run() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1c83ac53590>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
